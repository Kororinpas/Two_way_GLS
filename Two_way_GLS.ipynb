{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Consider the two-way random effects model given in (6.13) of Ch.6 lecture notes:The regressors matrix X can contain time-invariant and individual-invariant regressors. The two-way random effects estimation follows the following iterative algorithm: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Write a Python script that implements the above algorithm, and apply it to estimate. The output should be in a table form containing the following columns of results: parameter estimates, standard errors, t\u0002statistics and p-values. The table should also have row and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data and create Z_state and Z_yr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('cigar.xlsx')\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna()\n",
    "### These dummies are used to create the Q matrix\n",
    "Z_STATE = pd.get_dummies(df['STATE'])\n",
    "Z_YR = pd.get_dummies(df['YR'])\n",
    "\n",
    "### This dummy are used to include in X, we drop first in order to avoid multicollinearity\n",
    "Z_YR_1 = pd.get_dummies(df['YR'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose the dependent variable and the independent variables\n",
    "Y = np.log(df['C'])\n",
    "df['ln_Price'] = np.log(df['Price'])\n",
    "df['ln_NDI'] = np.log(df['NDI'])\n",
    "df['ln_PIMIN'] = np.log(df['PIMIN'])\n",
    "X = df[['ln_Price','ln_NDI','ln_PIMIN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the X matrix; we drop the constant and only drop the first dummy in Year dummy\n",
    "Random_X = pd.concat([X,Z_STATE,Z_YR_1],axis=1)\n",
    "Random_X = np.array(Random_X,dtype=float)\n",
    "Y = np.array(Y,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for the previous prepared data\n",
    "\n",
    "# Get the beta and the residual\n",
    "beta = np.linalg.inv(Random_X.T @ Random_X) @ Random_X.T @ Y\n",
    "residual = Y - Random_X @ beta\n",
    "\n",
    "\n",
    "### Estimate the variance of the u_hat and update\n",
    "def update_the_variance_matrix(residual,Z_STATE,Z_YR):\n",
    "    ##compute the w_hat\n",
    "    def get_w_hat(Q,u_hat):\n",
    "        numerator = u_hat.T @ Q @ u_hat\n",
    "        denominator = np.trace(Q)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    ## Preapre for the matrix for computing the variance matrix\n",
    "    Z_STATE_Z_STATE = Z_STATE @ Z_STATE.T\n",
    "    Z_YR_Z_YR = Z_YR @ Z_YR.T\n",
    "\n",
    "    J_n = np.ones(Z_STATE.shape[1]).reshape(Z_STATE.shape[1],1)@np.ones(Z_STATE.shape[1]).reshape(Z_STATE.shape[1],1).T\n",
    "    J_t = np.ones(Z_YR.shape[1]).reshape(Z_YR.shape[1],1)@np.ones(Z_YR.shape[1]).reshape(Z_YR.shape[1],1).T\n",
    "\n",
    "\n",
    "    bar_J_n = (1/(Z_STATE.shape[1])) * J_n\n",
    "    bar_J_t = (1/(Z_STATE.shape[1])) * J_t\n",
    "\n",
    "\n",
    "    E_n = np.eye(Z_STATE.shape[1]) - bar_J_n\n",
    "    E_t = np.eye(Z_YR.shape[1]) - bar_J_t\n",
    "\n",
    "    ### estimate the Q matrix\n",
    "    Q_1 = np.kron(E_n,E_t)\n",
    "    Q_2 = np.kron(E_n,bar_J_t)\n",
    "    Q_3 = np.kron(bar_J_n,E_t)\n",
    "    Q_4 = np.kron(bar_J_n,bar_J_t)\n",
    "\n",
    "    #estimate the w_hat\n",
    "    w_1 = get_w_hat(Q_1,residual)\n",
    "    w_2 = get_w_hat(Q_2,residual)\n",
    "    w_3 = get_w_hat(Q_3,residual)\n",
    "    w_4 = get_w_hat(Q_4,residual)\n",
    "\n",
    "\n",
    "    #compute the variance matrix\n",
    "    variance_matrix = w_1*Q_1 + w_2*Q_2 + w_3*Q_3 + w_4*Q_4\n",
    "\n",
    "    return variance_matrix\n",
    "\n",
    "variance_matrix = update_the_variance_matrix(residual,Z_STATE,Z_YR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLS procedure\n",
    "\n",
    "### GLS procedure using linalg.pinv, this works well even you have multicollinearity\n",
    "def GLS_pseudoinverse_procedure(X,Y,variance_matrix):\n",
    "    beta_gls = np.linalg.pinv(X.T @ np.linalg.pinv(variance_matrix) @ X) @ X.T @ np.linalg.pinv(variance_matrix) @ Y\n",
    "    residual_gls = Y - X @ beta_gls\n",
    "\n",
    "    return beta_gls,residual_gls\n",
    "\n",
    "### Standard GLS procedure using np.linalg.inv\n",
    "def GLS_procedure(X,Y,variance_matrix):\n",
    "    beta_gls = np.linalg.inv(X.T @ np.linalg.inv(variance_matrix) @ X) @ X.T @ np.linalg.inv(variance_matrix) @ Y\n",
    "    residual_gls = Y - X @ beta_gls\n",
    "\n",
    "    return beta_gls,residual_gls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.02619933  0.52720064 -0.09843756  4.52655017]\n",
      "[-1.02562187  0.5274804  -0.09809581  4.52187892]\n",
      "[-1.02551127  0.52747885 -0.09816123  4.5217926 ]\n",
      "[-1.02549626  0.5274769  -0.09817426  4.52180663]\n",
      "[-1.02549436  0.52747658 -0.09817609  4.52180951]\n",
      "[-1.02549413  0.52747653 -0.09817632  4.52180993]\n",
      "[-1.0254941   0.52747653 -0.09817635  4.52180997]\n",
      "[-1.0254941   0.52747653 -0.09817635  4.52180998]\n",
      "[-1.0254941   0.52747653 -0.09817635  4.52180997]\n",
      "[-1.0254941   0.52747652 -0.09817635  4.52180998]\n"
     ]
    }
   ],
   "source": [
    "## The reuslt of standard GLS procedure, after drop the first term in the Year dummy, the coefficient converges.\n",
    "for i in range(10):\n",
    "    beta,residual = GLS_procedure(X=Random_X,Y=Y,variance_matrix=variance_matrix)\n",
    "    variance_matrix = update_the_variance_matrix(residual=residual,Z_STATE=Z_STATE,Z_YR=Z_YR)\n",
    "    estimate_u_hat = (residual.T@residual)/(Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "    print(beta[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01642193  0.51687575 -0.12936621  1.97174798]\n",
      "[-1.02972137  0.52742002 -0.0956016   1.90634123]\n",
      "[-1.02718653  0.52765969 -0.09677998  1.9046346 ]\n",
      "[-1.02575384  0.52751593 -0.09793718  1.90489467]\n",
      "[-1.02552722  0.52748235 -0.09814398  1.90500249]\n",
      "[-1.02549818  0.52747729 -0.09817226  1.90502142]\n",
      "[-1.0254946   0.52747662 -0.09817585  1.90502403]\n",
      "[-1.02549416  0.52747654 -0.09817629  1.90502437]\n",
      "[-1.02549411  0.52747653 -0.09817635  1.90502441]\n",
      "[-1.0254941   0.52747653 -0.09817635  1.90502441]\n"
     ]
    }
   ],
   "source": [
    "## The GLS procedure using linalg.pinv also converges to the same result as the previous GLS procedure\n",
    "for i in range(10):\n",
    "    beta,residual = GLS_pseudoinverse_procedure(X=Random_X,Y=Y,variance_matrix=variance_matrix)\n",
    "    variance_matrix = update_the_variance_matrix(residual=residual,Z_STATE=Z_STATE,Z_YR=Z_YR)\n",
    "    estimate_u_hat = (residual.T@residual)/(Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "    print(beta[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              beta        se      t_stat           pval\n",
      "ln_Price -1.025494  0.003034 -338.004087   0.000000e+00\n",
      "ln_NDI    0.527477  0.003373  156.359857   0.000000e+00\n",
      "ln_PIMIN -0.098176  0.003913  -25.087521  1.344413e-113\n"
     ]
    }
   ],
   "source": [
    "### Show the final GLS result\n",
    "import scipy.stats as stats\n",
    "beta_variance = np.linalg.inv(Random_X.T @ np.linalg.inv(variance_matrix) @ Random_X)*estimate_u_hat\n",
    "se = np.sqrt(np.diag(beta_variance))\n",
    "t_stat = beta/se\n",
    "pval = 2 * stats.t.cdf(-abs(t_stat),df=Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "print(pd.DataFrame({'beta':beta[:3],'se':se[:3],'t_stat':t_stat[:3],'pval':pval[:3]},index=['ln_Price','ln_NDI','ln_PIMIN']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Extend the estimation in part (a) by including correlated random effects for ùúáùëñ using the  time averages of ln ùëÉriceùëñt, ln ùëÅDIùëñt, and ln ùëÉIMINùëñt as additional regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.02446729  0.52331137 -0.10558725 -0.41096574]\n",
      "[-1.01900125  0.51697335 -0.12342188 -0.46560558]\n",
      "[-1.01844049  0.51634015 -0.12515941 -0.47066404]\n",
      "[-1.01838295  0.51627791 -0.12532612 -0.47110746]\n",
      "[-1.01837687  0.5162716  -0.12534262 -0.47114698]\n",
      "[-1.01837621  0.51627094 -0.12534431 -0.4711506 ]\n",
      "[-1.01837614  0.51627087 -0.12534449 -0.47115095]\n",
      "[-1.01837613  0.51627086 -0.12534451 -0.47115098]\n",
      "[-1.01837613  0.51627086 -0.12534451 -0.47115098]\n",
      "[-1.01837613  0.51627086 -0.12534451 -0.47115098]\n"
     ]
    }
   ],
   "source": [
    "df['region_ln_price'] = df.groupby('STATE')['ln_Price'].transform('mean')\n",
    "df['region_ln_NDI'] = df.groupby('STATE')['ln_NDI'].transform('mean')\n",
    "df['region_ln_PIMIN'] = df.groupby('STATE')['ln_PIMIN'].transform('mean')\n",
    "\n",
    "X = df[['ln_Price','ln_NDI','ln_PIMIN','region_ln_price','region_ln_NDI','region_ln_PIMIN']]\n",
    "Random_X = pd.concat([X,Z_YR],axis=1)\n",
    "Random_X = np.array(Random_X,dtype=float)\n",
    "\n",
    "beta = np.linalg.inv(Random_X.T @ Random_X) @ Random_X.T @ Y\n",
    "residual = Y - Random_X @ beta\n",
    "\n",
    "for i in range(10):\n",
    "    beta,residual = GLS_pseudoinverse_procedure(X=Random_X,Y=Y,variance_matrix=variance_matrix)\n",
    "    variance_matrix = update_the_variance_matrix(residual=residual,Z_STATE=Z_STATE,Z_YR=Z_YR)\n",
    "    estimate_u_hat = (residual.T@residual)/(Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "    print(beta[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     beta        se     t_stat           pval\n",
      "ln_Price        -1.018376  0.015742 -64.690801   0.000000e+00\n",
      "ln_NDI           0.516271  0.017637  29.272280  4.226539e-146\n",
      "ln_PIMIN        -0.125345  0.020429  -6.135498   1.114999e-09\n",
      "region_ln_price -0.471151  0.020607 -22.863214   5.519484e-98\n",
      "region_ln_NDI    0.089092  0.018465   4.824813   1.561444e-06\n",
      "region_ln_PIMIN  0.447316  0.023642  18.920719   5.770675e-71\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "beta_variance = np.linalg.inv(Random_X.T @ np.linalg.inv(variance_matrix) @ Random_X)*estimate_u_hat\n",
    "se = np.sqrt(np.diag(beta_variance))\n",
    "t_stat = beta/se\n",
    "pval = 2 * stats.t.cdf(-abs(t_stat),df=Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "print(pd.DataFrame({'beta':beta[:6],'se':se[:6],'t_stat':t_stat[:6],'pval':pval[:6]},index=['ln_Price','ln_NDI','ln_PIMIN','region_ln_price','region_ln_NDI','region_ln_PIMIN']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Further extend the estimation by including the three regressors Pop, Pop16 and CPI in the model. Compare the results with those in part (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01628644  0.52344955 -0.1257655  -0.40813067]\n",
      "[-1.01772732  0.52682486 -0.11633606 -0.37437203]\n",
      "[-1.01844142  0.52824331 -0.11238419 -0.36177168]\n",
      "[-1.01876262  0.52885157 -0.11068947 -0.35658888]\n",
      "[-1.01890413  0.52911538 -0.10995437 -0.35437681]\n",
      "[-1.01896628  0.5292305  -0.10963344 -0.35341788]\n",
      "[-1.01899351  0.52928087 -0.10949313 -0.35299941]\n",
      "[-1.01900545  0.52930289 -0.1094316  -0.35281622]\n",
      "[-1.01901069  0.52931255 -0.10940464 -0.35273595]\n",
      "[-1.01901296  0.52931683 -0.10939282 -0.3527008 ]\n"
     ]
    }
   ],
   "source": [
    "X = df[['ln_Price','ln_NDI','ln_PIMIN','region_ln_price','region_ln_NDI','region_ln_PIMIN','Pop','Pop16','CPI']]\n",
    "Random_X = pd.concat([X,Z_YR],axis=1)\n",
    "Random_X = np.array(Random_X,dtype=float)\n",
    "\n",
    "beta = np.linalg.inv(Random_X.T @ Random_X) @ Random_X.T @ Y\n",
    "residual = Y - Random_X @ beta\n",
    "\n",
    "for i in range(10):\n",
    "    beta,residual = GLS_pseudoinverse_procedure(X=Random_X,Y=Y,variance_matrix=variance_matrix)\n",
    "    variance_matrix = update_the_variance_matrix(residual=residual,Z_STATE=Z_STATE,Z_YR=Z_YR)\n",
    "    estimate_u_hat = (residual.T@residual)/(Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "    print(beta[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     beta          se     t_stat           pval\n",
      "ln_Price        -1.019013    0.015504 -65.723738   0.000000e+00\n",
      "ln_NDI           0.529317    0.017247  30.689531  4.347136e-157\n",
      "ln_PIMIN        -0.109393    0.020004  -5.468655   5.403537e-08\n",
      "region_ln_price -0.352701    0.020331 -17.348145   5.624227e-61\n",
      "region_ln_NDI    0.127952    0.018180   7.037918   3.105761e-12\n",
      "region_ln_PIMIN  0.395541    0.023265  17.001839   7.485307e-59\n",
      "Pop             -0.000023    0.000003  -7.962938   3.557362e-15\n",
      "Pop16            0.000023    0.000004   5.852377   6.084912e-09\n",
      "CPI              0.039779  482.007474   0.000083   9.999342e-01\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "beta_variance = np.linalg.inv(Random_X.T @ np.linalg.inv(variance_matrix) @ Random_X)*estimate_u_hat\n",
    "se = np.sqrt(np.diag(beta_variance))\n",
    "t_stat = beta/se\n",
    "pval = 2 * stats.t.cdf(-abs(t_stat),df=Random_X.shape[0]-Random_X.shape[1]-1)\n",
    "\n",
    "print(pd.DataFrame({'beta':beta[:9],'se':se[:9],'t_stat':t_stat[:9],'pval':pval[:9]},index=['ln_Price','ln_NDI','ln_PIMIN','region_ln_price','region_ln_NDI','region_ln_PIMIN','Pop','Pop16','CPI']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can found that there is only little change on the coefficient and their statistical significance do not change so much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Estimate the model in part (a) by treating ùúáùëñ and ùúÜùë° as fixed effects. Compare the results with those in part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable |      Beta |       SE |     T-stat |           P-value |\n",
    "|----------|----------:|---------:|-----------:|------------------:|\n",
    "| ln_Price | -1.025494 | 0.003034 | -338.004087|       0.000000e+00 |\n",
    "| ln_NDI   |  0.527477 | 0.003373 |  156.359857|       0.000000e+00 |\n",
    "| ln_PIMIN | -0.098176 | 0.003913 |  -25.087521| 1.344413e-113      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                   ln_C   R-squared:                        0.3966\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.3139\n",
      "No. Observations:                1380   R-squared (Within):              -3.3929\n",
      "Date:                Wed, Mar 06 2024   R-squared (Overall):             -0.8845\n",
      "Time:                        16:36:53   Log-likelihood                    1664.2\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      285.26\n",
      "Entities:                          46   P-value                           0.0000\n",
      "Avg Obs:                       30.000   Distribution:                  F(3,1302)\n",
      "Min Obs:                       30.000                                           \n",
      "Max Obs:                       30.000   F-statistic (robust):             285.26\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                      30   Distribution:                  F(3,1302)\n",
      "Avg Obs:                       46.000                                           \n",
      "Min Obs:                       46.000                                           \n",
      "Max Obs:                       46.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "const          4.8826     0.5136     9.5073     0.0000      3.8751      5.8900\n",
      "ln_Price      -1.0231     0.0418    -24.465     0.0000     -1.1051     -0.9410\n",
      "ln_NDI         0.5200     0.0467     11.139     0.0000      0.4284      0.6116\n",
      "ln_PIMIN      -0.1172     0.0541    -2.1661     0.0305     -0.2234     -0.0111\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 80.917\n",
      "P-value: 0.0000\n",
      "Distribution: F(74,1302)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "from linearmodels.panel import PanelOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data_combined = df.set_index(['STATE', 'YR'])\n",
    "data_combined['ln_C'] = np.log(data_combined['C'])\n",
    "exog_vars = ['ln_Price','ln_NDI','ln_PIMIN']\n",
    "exog = sm.add_constant(data_combined[exog_vars])\n",
    "mod = PanelOLS(data_combined['ln_C'], exog, time_effects=True,entity_effects=True)\n",
    "panel_res = mod.fit()\n",
    "print(panel_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is almost the same, while there is some small change on the std error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Consider the estimation of the two-way random effects model in Problem 1 above again, but using the maximum likelihood approach. The partially maximized loglikelihood function of ùúô2 and ùúô3 takes the form:...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Explore and summarize how Python module scipy can help maximizing log ùêøùëê(ùúô2, ùúô32). You may like to refer to the SciPy Documentation available at https://docs.scipy.org.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). Get the parameter you use to minimize the likehood function\n",
    "\n",
    "(2). Express other variable into the variable of your parameter\n",
    "\n",
    "(3). Write down your likehood function\n",
    "\n",
    "(4). Choose a begin value of your parameter and range of your parameter\n",
    "\n",
    "(5). Minimize your likehood function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Write a Python script to implement the maximization of log ùêøùëê(ùúô2, ùúô3), or the minimization of ‚àílog ùêøùëê(ùúô22, ùúô32), based on the cigarette demand data: cigar.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### Prepare : Get X and Y\n",
    "df = pd.read_excel('cigar.xlsx')\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna()\n",
    "Z_STATE = pd.get_dummies(df['STATE'])\n",
    "Z_YR = pd.get_dummies(df['YR'])\n",
    "\n",
    "Z_STATE_1 = pd.get_dummies(df['STATE'],drop_first=True)\n",
    "\n",
    "Y = np.log(df['C'])\n",
    "df['ln_Price'] = np.log(df['Price'])\n",
    "df['ln_NDI'] = np.log(df['NDI'])\n",
    "df['ln_PIMIN'] = np.log(df['PIMIN'])\n",
    "X = df[['ln_Price','ln_NDI','ln_PIMIN']]\n",
    "\n",
    "X = pd.concat([X,Z_STATE_1,Z_YR],axis=1)\n",
    "X = np.array(X,dtype=float)\n",
    "Y = np.array(Y,dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out your parameter: phi_2; phi_3\n",
    "### Find out how to express the omega using phi_2 and phi_3\n",
    "### omega = w_1*Q_1 + w_2*Q_2 + w_3*Q_3 + w_4*Q_4\n",
    "\n",
    "## So we need to find out the Q matrix first as before\n",
    "\n",
    "\n",
    "J_n = np.ones(Z_STATE.shape[1]).reshape(Z_STATE.shape[1],1)@np.ones(Z_STATE.shape[1]).reshape(Z_STATE.shape[1],1).T\n",
    "J_t = np.ones(Z_YR.shape[1]).reshape(Z_YR.shape[1],1)@np.ones(Z_YR.shape[1]).reshape(Z_YR.shape[1],1).T\n",
    "\n",
    "\n",
    "bar_J_n = (1/(Z_STATE.shape[1])) * J_n\n",
    "bar_J_t = (1/(Z_STATE.shape[1])) * J_t\n",
    "\n",
    "\n",
    "E_n = np.eye(Z_STATE.shape[1]) - bar_J_n\n",
    "E_t = np.eye(Z_YR.shape[1]) - bar_J_t\n",
    "\n",
    "## estimate the Q matrix\n",
    "Q_1 = np.kron(E_n,E_t)\n",
    "Q_2 = np.kron(E_n,bar_J_t)\n",
    "Q_3 = np.kron(bar_J_n,E_t)\n",
    "Q_4 = np.kron(bar_J_n,bar_J_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kororinpa\\AppData\\Local\\Temp\\ipykernel_4296\\1082080352.py:44: RuntimeWarning: invalid value encountered in log\n",
      "  term_4 = (-1/2)*np.log(phi_2+phi_3-(phi_2*phi_3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: [2.26194218 1.79240975]\n"
     ]
    }
   ],
   "source": [
    "### Then express the w_i\n",
    "\n",
    "### w_2 = sigma_v^2/phi_2\n",
    "### w_3 = sigma_v^2/phi_3\n",
    "### w_1 = sigma_v^2\n",
    "### w_4 = w_2+w_3-w_1 = sigma_v^2/phi_2 + sigma_v^2/phi_3 - sigma_v^2\n",
    "### omega = w_1*Q_1 + w_2*Q_2 + w_3*Q_3 + w_4*Q_4 = sigma_v^2(Q1+1/phi_2*Q2+1/phi_3*Q3+(1/phi_2+1/phi_3-1)Q4)\n",
    "### Large_sigma = sigma_v^-2*omega = Q1+1/phi_2*Q2+1/phi_3*Q3+(1/phi_2+1/phi_3-1)Q4\n",
    "\n",
    "### Then we begin express the likelihood function\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "## We define the compute metrice function in order to express some variables using phi_2 and phi_3\n",
    "def compute_metrice(Q_1,Q_2,Q_3,Q_4,phi_2,phi_3,X,Y,n,t):\n",
    "    large_sigma = Q_1 + (1/phi_2)*Q_2 + (1/phi_3)*Q_3 + (1/phi_2+1/phi_3-1)*Q_4\n",
    "    large_sigma_inv = Q_1 + phi_2*Q_2 + phi_3*Q_3 + (phi_2*phi_3)/(phi_2+phi_3-(phi_2*phi_3))*Q_4\n",
    "    beta = np.linalg.inv(X.T @ large_sigma_inv @ X) @ X.T @ large_sigma_inv @ Y\n",
    "    mu = Y-X@beta\n",
    "    sigma_v = (1/n*t)*(Y-X@beta).T@large_sigma_inv@(Y-X@beta)\n",
    "\n",
    "    return beta,mu,sigma_v,large_sigma_inv\n",
    "\n",
    "\n",
    "## Then we define the likelihood function\n",
    "\n",
    "def neg_log_likelihood(params,Q_1, Q_2, Q_3, Q_4, Y, X,n,t):\n",
    "    phi_2 = params[0]\n",
    "    phi_3 = params[1]\n",
    "\n",
    "\n",
    "    beta,mu,sigma_v,large_sigma_inv = compute_metrice(Q_1=Q_1,\n",
    "                                          Q_2=Q_2,\n",
    "                                          Q_3=Q_3,\n",
    "                                          Q_4=Q_4,\n",
    "                                          phi_2=phi_2,\n",
    "                                          phi_3=phi_3,\n",
    "                                          X =X,\n",
    "                                          Y=Y,\n",
    "                                          n=n,\n",
    "                                          t=t)\n",
    "    ## constant will not change the reuslt, ignore it\n",
    "    term_1 = -((n*t)/2)*np.log(sigma_v)\n",
    "    term_2 = (n/2)*np.log(phi_2)\n",
    "    term_3 = (t/2)*np.log(phi_3)\n",
    "    term_4 = (-1/2)*np.log(phi_2+phi_3-(phi_2*phi_3))\n",
    "    term_5 = -(1/(2*sigma_v))*mu.T@large_sigma_inv@mu\n",
    "\n",
    "\n",
    "\n",
    "    nll = -(term_1+term_2+term_3+term_4+term_5)\n",
    "\n",
    "    return nll\n",
    "\n",
    "## In pratice, we find sometime term4 will be negative, so we need to add a constraint to make sure term4 is positive\n",
    "def constraint(params):\n",
    "    phi_2, phi_3 = params\n",
    "    return phi_2 + phi_3 - phi_2 * phi_3\n",
    "\n",
    "cons = ({'type': 'ineq', 'fun': constraint})\n",
    "\n",
    "\n",
    "\n",
    "### set the intial_params and bounds\n",
    "initial_params = [0.1,0.1]\n",
    "bounds = [(0.0001, None)]*2\n",
    "n= Z_STATE.shape[1]\n",
    "t=Z_YR.shape[1]\n",
    "## 'TNC', 'SLSQP', 'Nelder-Mead' can be used here, but we find the SLSQP is the best\n",
    "method = 'SLSQP'\n",
    "### optimize the function\n",
    "result = minimize(neg_log_likelihood, initial_params, args=(Q_1, Q_2, Q_3, Q_4, Y, X,n,t),method=method, bounds=bounds,constraints=cons)\n",
    "\n",
    "if result.success:\n",
    "    fitted_params = result.x\n",
    "    print('Optimal parameters:', fitted_params)\n",
    "else:\n",
    "    raise ValueError(result.message)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After getting the result, we can use the fitted_params to compute the beta, mu, sigma_v, large_sigma_inv\n",
    "beta,mu,sigma_v,large_sigma_inv = compute_metrice(Q_1=Q_1,Q_2=Q_2,Q_3=Q_3,Q_4=Q_4,phi_2=fitted_params[0],phi_3=fitted_params[0],X=X,Y=Y,n=n,t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable |      Beta |       SE |     T-stat |           P-value |\n",
    "|----------|----------:|---------:|-----------:|------------------:|\n",
    "| ln_Price | -1.025494 | 0.003034 | -338.004087|       0.000000e+00 |\n",
    "| ln_NDI   |  0.527477 | 0.003373 |  156.359857|       0.000000e+00 |\n",
    "| ln_PIMIN | -0.098176 | 0.003913 |  -25.087521| 1.344413e-113      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.02457955,  0.52564843, -0.10304916])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the parameters of ln_price, ln_ndi and ln_pimin here. After comparing it with the GLS results, we find the coefficient of them are really similar, which proves that this MLE procedure was a successful one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
